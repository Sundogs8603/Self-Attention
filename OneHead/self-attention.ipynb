{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 准备输入\n",
    "x = [\n",
    "      [1, 0, 1, 0], # Input 1\n",
    "      [0, 2, 0, 2], # Input 2\n",
    "      [1, 1, 1, 1]  # Input 3\n",
    "     ]\n",
    "x = torch.tensor(x, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# 初始化权重\n",
    "w_key = [\n",
    "      [0, 0, 1],\n",
    "      [1, 1, 0],\n",
    "      [0, 1, 0],\n",
    "      [1, 1, 0]\n",
    "    ]\n",
    "w_query = [\n",
    "      [1, 0, 1],\n",
    "      [1, 0, 0],\n",
    "      [0, 0, 1],\n",
    "      [0, 1, 1]\n",
    "    ]\n",
    "w_value = [\n",
    "      [0, 2, 0],\n",
    "      [0, 3, 0],\n",
    "      [1, 0, 3],\n",
    "      [1, 1, 0]\n",
    "    ]\n",
    "w_key = torch.tensor(w_key, dtype=torch.float32)\n",
    "w_query = torch.tensor(w_query, dtype=torch.float32)\n",
    "w_value = torch.tensor(w_value, dtype=torch.float32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1.],\n",
      "        [4., 4., 0.],\n",
      "        [2., 3., 1.]])\n",
      "tensor([[1., 0., 2.],\n",
      "        [2., 2., 2.],\n",
      "        [2., 1., 3.]])\n",
      "tensor([[1., 2., 3.],\n",
      "        [2., 8., 0.],\n",
      "        [2., 6., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# 导出key, query and value的表示\n",
    "keys = x @ w_key\n",
    "querys = x @ w_query\n",
    "values = x @ w_value\n",
    "print(keys)\n",
    "# tensor([[0., 1., 1.],\n",
    "#         [4., 4., 0.],\n",
    "#         [2., 3., 1.]])\n",
    "print(querys)\n",
    "# tensor([[1., 0., 2.],\n",
    "#         [2., 2., 2.],\n",
    "#         [2., 1., 3.]])\n",
    "print(values)\n",
    "# tensor([[1., 2., 3.],\n",
    "#         [2., 8., 0.],\n",
    "#         [2., 6., 3.]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 计算输入的注意力得分(attention scores)\n",
    "attn_scores = querys @ keys.T\n",
    "# tensor([[ 2.,  4.,  4.],  # attention scores from Query 1\n",
    "#         [ 4., 16., 12.],  # attention scores from Query 2\n",
    "#         [ 4., 12., 10.]]) # attention scores from Query 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#  计算softmax\n",
    "attn_scores_softmax = softmax(attn_scores, dim=-1)\n",
    "    # tensor([[6.3379e-02, 4.6831e-01, 4.6831e-01],\n",
    "    #         [6.0337e-06, 9.8201e-01, 1.7986e-02],\n",
    "    #         [2.9539e-04, 8.8054e-01, 1.1917e-01]])\n",
    "    # For readability, approximate the above as follows\n",
    "attn_scores_softmax = [\n",
    "      [0.0, 0.5, 0.5],\n",
    "      [0.0, 1.0, 0.0],\n",
    "      [0.0, 0.9, 0.1]\n",
    "]\n",
    "attn_scores_softmax = torch.tensor(attn_scores_softmax)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Step 6: 将attention scores乘以value\n",
    "\n",
    "weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n",
    "# tensor([[[0.0000, 0.0000, 0.0000],\n",
    "#          [0.0000, 0.0000, 0.0000],\n",
    "#          [0.0000, 0.0000, 0.0000]],\n",
    "#\n",
    "#         [[1.0000, 4.0000, 0.0000],\n",
    "#          [2.0000, 8.0000, 0.0000],\n",
    "#          [1.8000, 7.2000, 0.0000]],\n",
    "#\n",
    "#         [[1.0000, 3.0000, 1.5000],\n",
    "#          [0.0000, 0.0000, 0.0000],\n",
    "#          [0.2000, 0.6000, 0.3000]]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Step 7: 对加权后的value求和以得到输出\n",
    "outputs = weighted_values.sum(dim=0)\n",
    "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
    "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
    "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 2., 3.]],\n\n        [[2., 8., 0.]],\n\n        [[2., 6., 3.]]])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values[:, None]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.0000],\n         [0.0000],\n         [0.0000]],\n\n        [[0.5000],\n         [1.0000],\n         [0.9000]],\n\n        [[0.5000],\n         [0.0000],\n         [0.1000]]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_softmax.T[:,:,None]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000]],\n\n        [[1.0000, 4.0000, 0.0000],\n         [2.0000, 8.0000, 0.0000],\n         [1.8000, 7.2000, 0.0000]],\n\n        [[1.0000, 3.0000, 1.5000],\n         [0.0000, 0.0000, 0.0000],\n         [0.2000, 0.6000, 0.3000]]])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_values\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "下面来看看大的注意力分数乘以的value是不是和value余弦值最小"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [2., 8., 0.],\n",
      "        [2., 6., 3.]])\n",
      "tensor([[0.0000, 0.5000, 0.5000],\n",
      "        [0.0000, 1.0000, 0.0000],\n",
      "        [0.0000, 0.9000, 0.1000]])\n",
      "tensor([[2.0000, 7.0000, 1.5000],\n",
      "        [2.0000, 8.0000, 0.0000],\n",
      "        [2.0000, 7.8000, 0.3000]])\n"
     ]
    }
   ],
   "source": [
    "print(values)\n",
    "#tensor([[1., 2., 3.],\n",
    "#        [2., 8., 0.],\n",
    "#        [2., 6., 3.]])\n",
    "print(attn_scores_softmax)\n",
    "#tensor([[0.0000, 0.5000, 0.5000],\n",
    "#        [0.0000, 1.0000, 0.0000],\n",
    "#        [0.0000, 0.9000, 0.1000]])\n",
    "print(outputs)\n",
    "# tensor([[2.0000, 7.0000, 1.5000],  # Output 1\n",
    "#         [2.0000, 8.0000, 0.0000],  # Output 2\n",
    "#         [2.0000, 7.8000, 0.3000]]) # Output 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "attn_scores_softmax矩阵0.9代表第三个单词关注第二个单词\n",
    "可以看到他俩的余弦相似度为0.993，很相似。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9993)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(outputs[2],values[1], dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9706)"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(outputs[0],values[2], dim=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0.9166)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(outputs[2],values[2], dim=0)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}