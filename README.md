# Self-Attention（pytorch实现）
OneHead-Attention文件夹下是单头自注意力机制  
MultiHead-Attention文件夹下是多头自注意力机制
### 注意：无论单头还是多头都没有mask和dropout(后续补充......)
